"""Multi-Agent System (MAS) Actor for MARFT.

This module provides MAPPO (Multi-Agent PPO) implementation for multi-agent
reinforcement fine-tuning scenarios. It extends the standard PPO actor with
multi-agent specific advantage computation strategies.
"""

import functools
from typing import Any

import torch

from areal.api.cli_args import MicroBatchSpec, PPOActorConfig
from areal.api.engine_api import TrainEngine
from areal.engine.fsdp_engine import FSDPEngine
from areal.engine.megatron_engine import MegatronEngine
from areal.engine.ppo.actor import PPOActor, grpo_loss_fn
from areal.utils import logging, stats_tracker
from areal.utils.data import split_padded_tensor_dict_into_mb_list
from areal.utils.perf_tracer import trace_perf
from areal.utils.functional import reward_overlong_penalty

logger = logging.getLogger(__name__)


class MAPPOActor(PPOActor):
    """Multi-Agent PPO Actor for MARFT (Multi-Agent Reinforcement Fine-Tuning).

    Inherits all PPO functionality from PPOActor but uses a different advantage
    computation strategy suitable for multi-agent settings.

    Key differences from PPOActor:
    - Agent-aware advantage computation
    - Support for centralized critic with decentralized actors (CTDE)
    - Multi-agent credit assignment mechanisms
    - Coordinated exploration strategies
    """

    def __init__(self, config: PPOActorConfig, engine: TrainEngine):
        super().__init__(config, engine)
        # TODO(agent): Add MAPPO-specific initialization
        # Potential additions:
        # - self.centralized_critic: Optional critic network
        # - self.agent_specific_value_heads: Per-agent value functions
        # - self.credit_assignment_mode: How to distribute rewards among agents
        logger.info("Initialized MAPPOActor with multi-agent advantage computation")

    @trace_perf("mappo_actor.compute_advantages", category="compute")
    def compute_advantages(self, data: dict[str, Any]) -> dict[str, Any]:
        bs = data["input_ids"].shape[0]
        max_seqlen = data["input_ids"].shape[1]
        batch_indices = torch.arange(
            bs, device=data["input_ids"].device, dtype=torch.long
        )

        # Reward Penalty on length
        if self.config.overlong_reward_penalty:
            overlong_tokens = self.config.overlong_tokens
            overlong_penalty_factor = self.config.overlong_penalty_factor

            data = reward_overlong_penalty(
                data,
                overlong_tokens=overlong_tokens,
                overlong_penalty_factor=overlong_penalty_factor,
                max_response_length=self.config.max_new_tokens,
            )

        # Reward Scaling
        reward_score = data["rewards"]
        reward_score = (reward_score + self.reward_bias) * self.reward_scaling
        reward_score = torch.clip(
            reward_score, max=self.reward_clip, min=-self.reward_clip
        )
        if self.reward_norm:
            reward_score = self.reward_norm(reward_score)

        loss_mask = data["loss_mask"].float()
        loss_mask = torch.roll(loss_mask, shifts=-1, dims=-1)
        # Apply the mask to log probabilities.
        if not self.config.use_decoupled_loss and self.config.recompute_logprob:
            # Overwrite logprobs produced by the inference engine
            old_logp = data["logprobs"] = data["prox_logp"]
        else:
            old_logp = torch.roll(data["logprobs"], shifts=-1, dims=-1)
            if not self.config.use_decoupled_loss:
                # prox logp not available, use inferenced logp
                data["prox_logp"] = old_logp
        ref_logp = data.get("ref_logp", torch.zeros_like(old_logp))
        ref_logp *= loss_mask
        old_logp *= loss_mask

        # Compute KL-regularized rewards.
        attn_mask = data["attention_mask"]
        seqlens = attn_mask.sum(-1).long()
        seq_no_eos_mask = seqlens == attn_mask.shape[1]
        rewards = -self.kl_ctl * self.kl_estimator(old_logp, ref_logp)
        kl_rewards = rewards.clone()
        # KL rewards at the next token after eos is zero.
        rewards[batch_indices, seqlens - 1] = 0
        indices = torch.clip(seqlens - 2, min=0)
        if self.mask_no_eos_with_zero:
            rewards[batch_indices, indices] += torch.where(
                seq_no_eos_mask, 0, reward_score
            )
        else:
            rewards[batch_indices, indices] += reward_score

        if "values" not in data:
            values = torch.zeros_like(rewards)
        else:
            values = data["values"]
            
        has_successor = 'successor_values' in data and data['successor_values'] is not None

        if has_successor:
            advantages_reversed = []
            nextvalues = data['successor_values'][:, 0].to(device=values.device, dtype=torch.float32)
            loop_range = range(max_seqlen)
        else:
            advantages_reversed = [torch.zeros(bs, dtype=torch.float32, device=values.device)]
            nextvalues = values[:, max_seqlen - 1] * seq_no_eos_mask
            loop_range = range(max_seqlen - 1)

        lastgaelam = 0
        for t in reversed(loop_range):
            delta = rewards[:, t] + self.discount * nextvalues - values[:, t]
            newgaelam = delta + self.discount * self.gae_lambda * lastgaelam

            mask = loss_mask[:, t]
            nextvalues = nextvalues * (1 - mask) + values[:, t] * mask
            lastgaelam = lastgaelam * (1 - mask) + newgaelam * mask

            advantages_reversed.append(lastgaelam)

        # 反转并堆叠，得到 [A_0, ..., A_{T-2}, A_{T-1}]
        # 无论哪种情况，advantages 的维度都是 [bs, max_seqlen]
        advantages = torch.stack(advantages_reversed[::-1], dim=1)
        data["returns"] = advantages + values

        if self.adv_norm is not None:
            advantages = self.adv_norm(advantages, loss_mask)

        data["advantages"] = advantages
        data["kl_rewards"] = kl_rewards
        data["tot_rewards"] = rewards
        data["loss_mask"] = loss_mask
        data["logprobs"] = old_logp

        return data

    @trace_perf("mappo_actor.ppo_update", category="compute")
    @stats_tracker.scope_func_wrapper("mappo_actor")
    def ppo_update(self, data: dict[str, Any]) -> None:
        super().ppo_update(data)


class FSDPMAPPOActor(FSDPEngine):
    """FSDP backend wrapper for MAPPOActor.

    Provides distributed training support via FSDP2 for multi-agent scenarios.
    """

    def __init__(self, config: PPOActorConfig):
        super().__init__(config)
        self.actor = MAPPOActor(config, self)

    @torch.no_grad()
    def compute_logp(self, *args, **kwargs) -> torch.Tensor:
        """Compute log probabilities using the wrapped MAPPOActor."""
        return self.actor.compute_logp(*args, **kwargs)

    @torch.no_grad()
    def compute_advantages(self, *args, **kwargs) -> dict[str, Any]:
        """Compute multi-agent advantages using the wrapped MAPPOActor."""
        return self.actor.compute_advantages(*args, **kwargs)

    def ppo_update(self, *args, **kwargs) -> None:
        """Perform MAPPO update using the wrapped MAPPOActor."""
        self.actor.ppo_update(*args, **kwargs)


class MegatronMAPPOActor(MegatronEngine):
    """Megatron backend wrapper for MAPPOActor.

    Provides distributed training support via Megatron-LM for multi-agent scenarios.
    """

    def __init__(self, config: PPOActorConfig):
        super().__init__(config)
        self.actor = MAPPOActor(config, self)

    @torch.no_grad()
    def compute_logp(self, *args, **kwargs) -> torch.Tensor:
        """Compute log probabilities using the wrapped MAPPOActor."""
        return self.actor.compute_logp(*args, **kwargs)

    @torch.no_grad()
    def compute_advantages(self, *args, **kwargs) -> dict[str, Any]:
        """Compute multi-agent advantages using the wrapped MAPPOActor."""
        return self.actor.compute_advantages(*args, **kwargs)

    def ppo_update(self, *args, **kwargs) -> None:
        """Perform MAPPO update using the wrapped MAPPOActor."""
        self.actor.ppo_update(*args, **kwargs)